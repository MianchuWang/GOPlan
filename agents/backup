'''
class Uncertainty(GCSL):
    def __init__(self, **agent_params):
        super().__init__(**agent_params)
        self.K = 2
        self.latent_dim = 32
        
        self.autoencoder = AE(self.state_dim, self.ac_dim, self.goal_dim, latent_dim=self.latent_dim).to(device=self.device)
        self.ae_opt = torch.optim.Adam(self.autoencoder.parameters(), lr=5e-4)
        self.ae_q_net = q_network(self.latent_dim, 0, 0).to(device=self.device)
        self.ae_q_target = q_network(self.latent_dim, 0, 0).to(device=self.device)
        self.ae_q_target.load_state_dict(self.ae_q_net.state_dict())
        self.ae_q_opt = torch.optim.Adam(self.ae_q_net.parameters(), lr=1e-4)
        self.ae_q_training_steps = 0
        
        self.q_nets = [q_network(self.state_dim, self.ac_dim, self.goal_dim).to(device=self.device) for _ in range(self.K)]
        self.q_target_nets = [q_network(self.state_dim, self.ac_dim, self.goal_dim).to(device=self.device) for _ in range(self.K)]
        for k in range(self.K):
            self.q_target_nets[k].load_state_dict(self.q_nets[k].state_dict())
        self.q_net_opts = [torch.optim.Adam(self.q_nets[k].parameters(), lr=5e-4) for k in range(self.K)]
        self.q_training_steps = 0

    def train_models(self, batch_size=512):
        value_info = self.train_value_function(batch_size=batch_size)
        policy_info = self.train_policy(batch_size=batch_size)
        ae_info = {}#self.train_autoencoder(batch_size=batch_size)
        ae_value_info = {}#self.train_autoencoder_value_function(batch_size=batch_size)
        if self.q_training_steps % 40 == 0:
            for k in range(self.K):
                self.update_target_nets(self.q_nets[k], self.q_target_nets[k])
        if self.ae_q_training_steps % 40 == 0:
            self.update_target_nets(self.ae_q_net, self.ae_q_target)
        return {**value_info, **policy_info, **ae_info, **ae_value_info}

    def train_autoencoder_value_function(self, batch_size):
        states, actions, next_states, goals = self.sample_func(batch_size)
        achieved_goals = self.get_goal_from_state(next_states)
        rewards = self.compute_reward(achieved_goals, goals, None)[..., np.newaxis]
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)
        states_prep, actions_prep, next_states_prep, goals_prep = \
            self.preprocess(states=states, actions=actions, next_states=next_states, goals=goals)
        
        with torch.no_grad():
            _, next_actions = self.policy(next_states_prep, goals_prep)
            next_latent = self.autoencoder.encode(next_states_prep, next_actions, goals_prep)
            q_next_value = self.ae_q_target(next_latent, 
                                            torch.zeros(batch_size, 0).to(device=self.device),
                                            torch.zeros(batch_size, 0).to(device=self.device))
            target_q_value = rewards_tensor + (1-rewards_tensor) * self.discount * q_next_value
        latent = self.autoencoder.encode(states_prep, actions_prep, goals_prep)
        pred_q_value = self.ae_q_net(latent, 
                                     torch.zeros(batch_size, 0).to(device=self.device),
                                     torch.zeros(batch_size, 0).to(device=self.device))
        q_loss = ((target_q_value - pred_q_value) ** 2).mean()
        self.ae_q_opt.zero_grad()
        q_loss.backward()
        self.ae_q_opt.step()

        self.ae_q_training_steps += 1
        return {'ae_v_loss': q_loss.item(),
                'ae_pred_value': pred_q_value.mean().item(),
                'ae_target_value': target_q_value.mean().item()}


    def train_autoencoder(self, batch_size):
        states, actions, next_states, goals = self.sample_func(batch_size)
        states_prep, actions_prep, _, goals_prep = self.preprocess(states=states, actions=actions, goals=goals)
        recon = self.autoencoder(states_prep, actions_prep, goals_prep)
        sg = torch.cat([states_prep, actions_prep, goals_prep], dim=-1)
        recon_loss = torch.nn.functional.mse_loss(recon, sg)

        shuffle_seed = torch.randperm(batch_size)
        latent = self.autoencoder.encode(states_prep, actions_prep, goals_prep)
        shuffled_latent = latent[shuffle_seed]
        latent_distance = ((latent - shuffled_latent) ** 2).mean(dim=1)
        
        values = self.q_nets[0](states_prep, actions_prep, goals_prep)
        shuffled_values = values[shuffle_seed]
        value_distance = ((values - shuffled_values) ** 2).mean(dim=1)
        
        latent_loss = ((latent_distance - value_distance) ** 2).mean()
        
        loss = recon_loss + latent_loss
        self.ae_opt.zero_grad()
        loss.backward()
        self.ae_opt.step()
        return {'autoencoder': loss.item(),
                'latent_loss': latent_loss.item(),
                'recon_loss': recon_loss.item()}


    def train_value_function(self, batch_size):
        states, actions, next_states, goals = self.sample_func(batch_size)
        achieved_goals = self.get_goal_from_state(next_states)
        rewards = self.compute_reward(achieved_goals, goals, None)[..., np.newaxis] + 1
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)
        states_prep, actions_prep, next_states_prep, goals_prep = \
                        self.preprocess(states, actions, next_states, goals)
        
        _, target_actions = self.policy(next_states_prep, goals_prep)
        smooth_noise = torch.clamp(0.2 * torch.randn_like(target_actions), - 0.5, 0.5)
        target_actions = torch.clamp(target_actions + smooth_noise, -1, 1)

        with torch.no_grad():
            q_next_values = torch.zeros(self.K, batch_size, 1).to(device=self.device)
            for i in range(self.K):
                q_next_values[i] = self.q_target_nets[i](next_states_prep, 
                                                         target_actions, goals_prep)
            q_next_value = torch.min(q_next_values, dim=0)[0]
            target_q_value = rewards_tensor + (1-rewards_tensor) * self.discount * q_next_value
        
        for k in range(self.K):
            pred_q_value = self.q_nets[k](states_prep, actions_prep, goals_prep)
            q_loss = ((target_q_value - pred_q_value)**2).mean()
                
            self.q_net_opts[k].zero_grad()
            q_loss.backward()
            self.q_net_opts[k].step()
        
        self.q_training_steps += 1
        
        return {'q_loss': q_loss.item(), 
                'pred_value': pred_q_value.mean().item(), 
                'target_value': target_q_value.mean().item()}

    def update_target_nets(self, net, target_net):
        for param, target_param in zip(net.parameters(), target_net.parameters()):
            target_param.data.copy_(0.05 * param.data + 0.95 * target_param.data)

    def train_policy(self, batch_size):
        states, actions, next_states, goals = self.replay_buffer.sample(batch_size, her_prob=self.her_prob)
        states_prep, actions_prep, _, goals_prep = self.preprocess(states=states, actions=actions, goals=goals)
        
        _, gen_actions = self.policy(states_prep, goals_prep)
        q_values = self.q_nets[0](states_prep, gen_actions, goals_prep)
        policy_loss = - 2 * q_values.mean() + ((actions_prep - gen_actions) ** 2).mean()
        
        self.policy_opt.zero_grad()
        policy_loss.backward()
        self.policy_opt.step()
        
        return {'policy_loss': policy_loss.item()}
'''
